\documentclass[a4paper,12pt,twoside]{article}
\pagestyle{headings}
\usepackage{a4wide}
\usepackage{mathtools}
\usepackage[colorlinks,hyperfigures,backref,bookmarks,draft=false]{hyperref}

\title{Recommender Systems\\ Web Science}
\author{Josephine Rehak}

\begin{document}
\maketitle
\section{Introduction}
Through the growing amount of items in the world wide web preselection and suggestion of items becomes more and more of importance. These recommendations support for instance the discovery of pruchasable objects, of watchable movies or interesting news.\\
\\
The overall goal is to detect similar items of interest for a user with the help of informations about the user itself, about the items and out of informations of other participants. 
\cite{WebScience}\\
Different approaches to calculate such recommendations exist. So called Collaborative Filtering methods try to give predictions for a user based on collected data about the personal preferences with regard to the preferences of others. \\
\\
An approach for a Collaborative Filtering Method is the calculation of cosine similarity. With its help a movie recommender system will be constructed in this survey, which allows to predict ratings of a user for movies based on existing user ratings. As fundamental data the free available data set of GroupLens with 100k ratings was used. \cite{grouplens}
 

\section{Methodology}

The cosine similarity calculates with a dot product (1) the cosine of the angle between two vectors.
The angle indicates if two vectors incline in the same direction.
\begin{equation}
cos(A, B) = \frac { A \cdot  B}{|| A|| \cdot || B||}
\end{equation}
The resulting value intermediates between -1 and 1. A value of -1 predicts an opposition or an unsimilarity. A prediction of 1 on the other hand implies the highest possible similarity: $cos(0) = 1$. \\
\\
In case of the movie recommender system a vector consists of all ratings of a user. To emphazise the tendencies of a user to give for example often high or low ratings. These ratings are subtracted by the average rating of the user accordingly. This approach is called adjusted cosine similarity.\\  
This results in the following equation for the two movies $i$ and $j$:\\
\begin{equation}
sim (i,j) = 
\frac{
	\displaystyle\sum_{u \in U} (R_u,_i - \overline{R_u})(R_u,_j - \overline{R_u})
}
{
	\sqrt{ \displaystyle\sum_{u \in U} (R_u,_i - \overline{R_u})^2} 
	* \sqrt{\displaystyle\sum_{u \in U} (R_u,_j - \overline{R_u})^2}
}
\end{equation}
$ i,j$ ... the two movies to compare \\
$\overline{R}(u)$ ... the average over all ratings of user $u$ \\
$R_u,_i$ , $R_u,_j$  ... ratings given by user $u$ to items $i$ and $j$\\
$U$ ... the set of users which rated both items $i$ and $j$\\
\\
To obtain the step from model to prediction a weighthed sum is used by multiplying the ratings of already rated films by a user with the corresponding cosine similarity. Furtheron this sum is divided by the sum of the absolute simlarity. \\ 
As a set of similar movies or items in this approach other movies rated by the same user were used.\cite{sarwar} 
\begin{equation}
P_u,_i = \frac{
\displaystyle\sum_{n \in N}(R_u,_n*sim(i,n))
}{
\displaystyle\sum_{n \in N}|sim(i,n)|
}
\end{equation}
$P_u,_i$ ... predicted rating of user $u$ for movie $i$\\
$R_u,_n$ ... rating of user $u$ for similar movies $n$\\
$sim(i,n)$ ... cosine similarity of movie $i$ and similar movie $n$\\
\\
This prediction results in values of the range -5 to 5. For further application the range needs to be translated to 1 to 5 which is the range of the data set. \\
Therefore the following equation can be applied:\\
\begin{equation}
\begin{split}
V(new) & = \frac{ V(old) - V(old)_{min}}{V(old)_{max} - V(old)_{min}} * (V(new)_{max} - V(new)_{min}) + V(new)_{min}\\
& = \frac{ 2* V(old)}{ 5} + 3
\end{split}
\end{equation}
$V(new)$ ... the value in the new range\\
$V(old)$ ... the value in the old range\\
$V(...)_{max}, V(...)_{min}$ ... min and max values of the range for the value\\
\\
The created recommender system can be evaluated by using the root-mean-square error (RMSE) procedure. 
\begin{equation}
RMSE =  \sqrt{\frac{\displaystyle\sum_{(i,j)}^{N}(p(u_i, m_j) - r(u_i,m_j))^2 }{n}}
\end{equation}
$ p(u_i, m_j) $ ... predicted value for user $i$ and movie $j$ \\
$ r(u_i,m_j) $ ... true value for user $i$ and movie $j$\\
\\
It can only be used for models using the same unit, therefore the normalisation of the prediction was used.
The RMSE value is the square root of the variance of the residuals so the lower the RMSE value, the better is the fit of prediction and the actual value. \cite{rsme}\\
\\
For better studies a value $k$ was introduced. $k$ ratings of the data set for each user were taken and transferred to a new set, called the test set. The remaining set forms the training set. \\ 
Through this step the effect of different sizes of the data set can be examined.


\section{About the Implementation}
For my project I use a local MySQL database in combination with a Python programm. To run the recommender.py file it might be necessary to download and install the MySQL server (\url{https://dev.mysql.com/downloads/installer/}) and a connector to allow Python the communication with the database (\url{https://dev.mysql.com/downloads/connector/python/2.1.html}). Furtheron Python should be installed.\\
\\
To execute the programm the data file u.data should be placed in the same directory as recommender.py and the MySQL server settings for host, username, password, database in the recommender.py file should be adapted.\\
The file loads the data of the u.data file into the database. To avoid repeated creation and deletion of the database tables the value of the variable requireDBinit ( line 5) should be set to False. \\
In this way unnecessary db access for the construction of tables and data are avoided. In case of changes to the database initialisation algorithm, a change of $k$ or new elements in the $u.data$ file it is mandatory to set the variable requireDBInit on True.\\
\\
The mentioned value k is reflected in the variable testSetK and is used for creating the database tables.\\
The request happen through the functions $calcCosSimilarity()$ and $calcPrediction()$ in the main functions. $calcCosSimilarity()$ was implemented with an extensive SQL statement to receive the preferred values from the database directly. \\
\\
The approach to implement all calculations via SQL was tried, but recognized as very unpractical. Therefore the slower, but less complex alternative of using Python for calculation of prediction and RMSE had been applied. Due to time pressure the run time of the new approach could not be optimized. \\
During the new calculations the case that a division by zero might occur. In this case the equation results in the number 0. \\


\section{Findings}

The cosine similarity is a collaborative filtering method so a dependency on the amount of ratings exists. \\
The use of the variable k allows to work with data sets of different sizes. \\
A $k$ of 3 would for example create a table consisting of the first 3 ratings of each user. \\
For comparison the film with movieID 1: "ToyStory", it is a childrens adventure movie, and the second movie with movieID 2: "GoldenEye", which is a mature action and thriller movie, were used.\\
Furtheron two similar movies , with movieID 144: "Die Hard" and movieID:226 "Die Hard 2" were chosen, which are suspected as alike.

\begin{tabular}{c|cc}
\textbf{k} & \textbf{similarity( 1 , 2 )} & \textbf{similarity ( 144, 226  )} \\ \hline
	2 & 1.0 & -1.0 \\
	\textbf{3} & 1.0 & 0.6278 \\
	4 & 1.0 & 0.6279 \\
	5 & 0.2944 & -0.1160 \\
	6 & 0.2944 & -0.1187\\
	7 & 0.3072 & 0.3411\\
	8 & -0.6821 &  0.3376 \\
	9 & -0.6821 & 0.3377\\
	\textbf{10} & -0.7681 & 0.3350\\
	20 & -0.8111 & 0.4781 \\
	* & -0.9506 & 0.2693 \\
\end{tabular} \\
\\
With a low $k$ value the similarity of the two unsimilar movies 1 and 2 is almost 1, which is the highest similarity possible, but decreases. The bigger $k$ becomes the smaller the cosine similarity gets.\\
The second example with two similar movies shows the same effects. While with a low $k$ value the similarity is low, it's increase leads to a higher calculated similarity.\\
This effect might have different reasons:\\
\begin{itemize}
\item The calculation for the similarity is based on an insufficient amount of ratings. For the cosine similarity calculation are pairs of two specific movies voted by the same user required. If none or few of such pairs are available, no propper calculations are possible.\\
\item With a low amount of ratings, each single rating has a higher effect on the similarity. A lack of higher variety ratings which relativize perculiar ratings leads to a biased calculated similarity.
\end{itemize}

Dependent on the development of the cosine similarity the RMSE value changes as well. Here the calculated RMSE value is shown in dependance to variable $k$:\\
\\
\begin{tabular}{c|c}
\textbf{k} & \textbf{RMSE} \\ \hline
	2 &  1,3967\\
	\textbf{3} & 1,5748 \\
	4 & 1,5800\\
	5 &  1,5256 \\
	6 &  1,4880\\
	7 & 1,4534\\
	8 &  1,4104  \\
	9 &  1,3665\\
	\textbf{10} & 1.3481 \\
	20 &  1.2585\\
	... & ... \\
\end{tabular} \\
\\
The bigger the data set becomes the smaller the calculated RMSE value becomes. The RMSE value is the square root of the average deviation of all predictions and actual ratings. It's decrease indicates better predictions of the recommender system.\\
This decrease of deviation can be explained by the growing amount of ratings on which the calculation is based.\\   
\\
All in all the here applied method of using a $k$ value shows that more training data improves the cosine similarity calculations as the predictions of the recommender system.	 


\section{Discussion}
During the survey different problems and weaknesses were encountered, which leave room for improvements.\\
The survey based on 100.000 entries, so the question arises if the user of bigger data sets would still be valid.\\
\\
The used formula construct of advanced cosine similarity, weighted sum and following normalisation as well as the use of the evaluation with RMSE is a functionable approach.\\
A growing amount of data should improve the predictions of the recommender system, because deviating ratings get relativized by the mass and more content for information extraction should be available.\\ 
\\
Adding new entries, which is mandatory for a growing data set, arises the problem of insufficient data to create predictions for these new items. A new movie can not be included in the system, when ratings for cosine similarity ratings are missing. This problem is the so called cold start problem of Collaborative Filtering.\\
The most famous approach to tackle this problem is the combination of a Collaborative Filtering method and a Content Based Matching methods. Content Based Matching allowes the calculation of a prediction by regarding the content similarity of a new item to rated items in the user preference profile. The latter could for example consist of items the user sighted before. If all these informations are missing the application of an initial profile with standard recommendations might be useful.\\
\\
Furtheron the problem of data sparsity should be considered. Not only the amount of items or users should grow, but the amount of ratings of users for the movie items. These ratings require engagement of the user and maintainer of the recommender system alone is the service not improvable.\\
Of course the point of required computation time should be considered. The more data is available, the more calculations are necessary.\\
\\
The SQL-based implementation is capable of more data entries. The request query for the cosine similarity remains with the growing amount of entries the same. As a database it is dimensioned for bigger amounts of data, the only limiting factor is the primary key for the table, which supports up to 18446744073709551615 entries. \cite{sql} The access latency becomes higher with the size of the data set.\\
\\
A huge disadvantage of SQL is the missing transparency. It is hard to detect the impact of the complex statements on the tables and to verify the request made. With more entries in the database it might become even more confusing.\\
\\
The Python part of the implementation is not optimized for the work with a higher amount of data. A vectorisation could be used for more efficient and faster calculations. 

\section{Conclusion}
The combination of cosine similarity and a weighted sum prediction was successfully implemented. \\
The survey on the size of the mandatory data set has shown that advanced cosine similarity and the following prediction work good in combination with the procedure of weighted sum. This approach is already common for recommender systems.\\
The implemented recommender system is not efficient. Furtheron does it not handle the cold start problem of recommender systems. A hybrid approach of Collective Filterings methods and Content Matchings methods could therefore be used.\\ 


\bibliographystyle{alpha}
\bibliography{task1.bib} 
\end{document}
